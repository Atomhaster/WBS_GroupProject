{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os, random\n",
    "\n",
    "from modules.painting import painting\n",
    "from modules.database import database as db\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.core import (Dropout, Flatten, Dense, Activation)\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.layers import (RandomFlip, RandomRotation, Input, BatchNormalization, \n",
    "                          RandomTranslation, RandomZoom)\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import Adam\n",
    "from keras.applications.vgg19 import VGG19, preprocess_input\n",
    "\n",
    "from tensorflow import unique_with_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Control Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting path variables for Working Directory and folder to save and load\n",
    "#   models from.\n",
    "wd = os.getcwd()\n",
    "WD_PATH =  os.path.abspath(wd)\n",
    "PATH_TRAINING = os.path.join(WD_PATH, \"model_training\")\n",
    "\n",
    "# # initializing the database object\n",
    "gallery = db()\n",
    "\n",
    "# Total number of paintings available\n",
    "TOTAL_PAINTINGS = 3971\n",
    "# Size of the pictures when reduced in size\n",
    "PIXEL_SIZE = 250\n",
    "\n",
    "# the proportions to split the available paintings in training\n",
    "# and testing.\n",
    "keep_unused = 10\n",
    "prop_train = 0.8\n",
    "prop_test = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[877 439 336 291 259 239 702 262 255 311]\n"
     ]
    }
   ],
   "source": [
    "# checking the available paintings per artist.\n",
    "num_paintings = np.zeros(10,np.int16)\n",
    "for j in range(10):\n",
    "    num_paintings[j] = len(gallery.get_paintingids_from_artist(j+1))\n",
    "print(num_paintings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 4.5279361459521095,\n",
       " 1: 9.045558086560364,\n",
       " 2: 11.818452380952381,\n",
       " 3: 13.646048109965635,\n",
       " 4: 15.332046332046332,\n",
       " 5: 16.615062761506277,\n",
       " 6: 5.656695156695156,\n",
       " 7: 15.15648854961832,\n",
       " 8: 15.572549019607843,\n",
       " 9: 12.768488745980708}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Weights for the model:\n",
    "class_temp = TOTAL_PAINTINGS / num_paintings\n",
    "class_weights = {}\n",
    "for i in range(10):\n",
    "    class_weights[i] = class_temp[i]\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total train paintings: 3092\n",
      "total test paintings: 770\n"
     ]
    }
   ],
   "source": [
    "# coding the sizes needed as collection arrays for the input data\n",
    "SIZE_P_TRAINING = 0\n",
    "training_size_paintings = num_paintings.copy()\n",
    "SIZE_P_TESTING = 0\n",
    "testing_size_paintings = num_paintings.copy()\n",
    "for i in range(10):\n",
    "    train_count = int((num_paintings[i] - keep_unused)  * prop_train)\n",
    "    test_count = int((num_paintings[i] - keep_unused)  * prop_test)\n",
    "    # print(sum((train_count,test_count,10)),training_size_paintings[i])\n",
    "    SIZE_P_TRAINING += train_count\n",
    "    SIZE_P_TESTING += test_count\n",
    "    training_size_paintings[i] = train_count\n",
    "    testing_size_paintings[i] = test_count\n",
    "print(\"total train paintings:\", SIZE_P_TRAINING)\n",
    "print(\"total test paintings:\", SIZE_P_TESTING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading data from DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3092, 250, 250, 3)\n",
      "(770, 250, 250, 3)\n",
      "(3092,)\n",
      "(770,)\n",
      "(109,)\n"
     ]
    }
   ],
   "source": [
    "## creating arrays to hold the pictures taken from the DB\n",
    "training_images = np.zeros((SIZE_P_TRAINING,PIXEL_SIZE,PIXEL_SIZE,3))\n",
    "index_training = 0\n",
    "skip_count_training = 0\n",
    "testing_images = np.zeros((SIZE_P_TESTING,PIXEL_SIZE,PIXEL_SIZE,3))\n",
    "index_testing = 0\n",
    "skip_count_testing = 0\n",
    "# creating the arrays to hold labels. In this case they are the artist ids.\n",
    "training_labels = np.array([0]*SIZE_P_TRAINING,dtype=int)\n",
    "testing_labels = np.array([0]*SIZE_P_TESTING,dtype=int)\n",
    "unused_paintings = np.array(\n",
    "    [0]*(TOTAL_PAINTINGS-SIZE_P_TESTING-SIZE_P_TRAINING)\n",
    "    ,dtype=int)\n",
    "# checking if the numbers add up.\n",
    "print(training_images.shape)\n",
    "print(testing_images.shape)\n",
    "print(training_labels.shape)\n",
    "print(testing_labels.shape)\n",
    "print(unused_paintings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop over the artists and adding randomized pictures of them to the <br>\n",
    "training and testing sets. There are also ids collected, which are unused, <br>\n",
    "so that they can be used as \"new, unseen\" input for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Artist\n",
      "Training images status\n",
      "690\n",
      "3\n",
      "New Artist\n",
      "Training images status\n",
      "1017\n",
      "19\n",
      "New Artist\n",
      "Training images status\n",
      "1277\n",
      "19\n",
      "New Artist\n",
      "Training images status\n",
      "1430\n",
      "90\n",
      "New Artist\n",
      "Training images status\n",
      "1628\n",
      "91\n",
      "New Artist\n",
      "Training images status\n",
      "1810\n",
      "92\n",
      "New Artist\n",
      "Training images status\n",
      "2363\n",
      "92\n",
      "New Artist\n",
      "Training images status\n",
      "2564\n",
      "92\n",
      "New Artist\n",
      "Training images status\n",
      "2759\n",
      "93\n",
      "New Artist\n",
      "Training images status\n",
      "2999\n",
      "93\n"
     ]
    }
   ],
   "source": [
    "# filling the arrays with picture arrays. They will be resized according\n",
    "# to the pixel_size value\n",
    "unused_index = 0\n",
    "for i in range(10):\n",
    "    # loading all ids from the artist\n",
    "    ids = gallery.get_paintingids_from_artist(i+1)\n",
    "    # shuffle the ids to get random order for selection\n",
    "    random.seed(1983)\n",
    "    random.shuffle(ids)\n",
    "    print(\"New Artist\")\n",
    "    \n",
    "    # getting the numbers for the current artist:\n",
    "    _str = training_size_paintings[i]\n",
    "    _ste = testing_size_paintings[i]\n",
    "    # slicing the ids for training and testing of artist with id i+1\n",
    "    ids_training = ids[ : _str]\n",
    "    ids_testing = ids[_str : _str + _ste]\n",
    "    ids_unused = ids[ _str + _ste : ]\n",
    "    \n",
    "    # collecting the ids of the unused paintings\n",
    "    for l, f in zip(range(unused_index,unused_index+len(ids_unused))\n",
    "                    , ids_unused):\n",
    "        unused_paintings[l] = f[0]\n",
    "    unused_index += len(ids_unused)\n",
    "    \n",
    "    # retrieving the paintings from the db, resizing them and collecting\n",
    "    # them in the training_images array while also filling the labels\n",
    "    for k in ids_training:\n",
    "        temp_p = painting(\"local DB\", id=k[0])\n",
    "        temp_p_res = cv.resize(temp_p.ndarray, dsize=(PIXEL_SIZE,PIXEL_SIZE)\n",
    "                               ,interpolation=cv.INTER_CUBIC)\n",
    "        if temp_p_res.shape == (PIXEL_SIZE,PIXEL_SIZE,3):\n",
    "            training_images[index_training] = temp_p_res\n",
    "            training_labels[index_training] = temp_p.artist_id-1\n",
    "            index_training += 1\n",
    "        else:\n",
    "            skip_count_training += 1\n",
    "            \n",
    "    # retrieving the paintings from the db, resizing them and collecting\n",
    "    # them in the testing_images array while also filling the labels\n",
    "    for j in ids_testing:\n",
    "        temp_p = painting(\"local DB\", id=j[0])\n",
    "        temp_p_res = cv.resize(temp_p.ndarray, dsize=(PIXEL_SIZE,PIXEL_SIZE)\n",
    "                               ,interpolation=cv.INTER_CUBIC)\n",
    "        if temp_p_res.shape == (PIXEL_SIZE,PIXEL_SIZE,3):\n",
    "            testing_images[index_testing] = temp_p_res\n",
    "            testing_labels[index_testing] = temp_p.artist_id-1\n",
    "            # testing_labels[index_testing] = [temp_p.artist_id-1,]\n",
    "            index_testing += 1\n",
    "        else:\n",
    "            skip_count_testing += 1\n",
    "\n",
    "# ## dropping the last few array positions of testing images, which where \n",
    "# ## not filled.\n",
    "testing_images = testing_images[:index_testing,:,:,:]\n",
    "testing_labels = testing_labels[:index_testing]\n",
    "# ## dropping the last few array positions of training images, which where \n",
    "# ## not filled.\n",
    "training_images = training_images[:index_training,:,:,:]\n",
    "training_labels = training_labels[:index_training]\n",
    "\n",
    "# # the pixels on an image are rescaled from 0-255 to 0-1 \n",
    "training_images, testing_images = training_images/255, testing_images/255 \n",
    "\n",
    "# defining labels in a list\n",
    "class_names = [i[1] for i in gallery.get_all_artists()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking data shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2999, 250, 250, 3)\n",
      "(746, 250, 250, 3)\n",
      "(2999,)\n",
      "(746,)\n",
      "<class 'numpy.ndarray'>\n",
      "(109,)\n"
     ]
    }
   ],
   "source": [
    "print(training_images.shape)\n",
    "print(testing_images.shape)\n",
    "print(training_labels.shape)\n",
    "print(testing_labels.shape)\n",
    "print(type(training_labels[185:200]))\n",
    "print(unused_paintings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Creating the weight vector for the artists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking data counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tl, idx_tl, count_tl = unique_with_counts(training_labels)\n",
    "y_testl, idx_testl, count_testl = unique_with_counts(testing_labels)\n",
    "y_unused, idx_unused, count_unused = unique_with_counts(unused_paintings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0 1 2 3 4 5 6 7 8 9], shape=(10,), dtype=int32) tf.Tensor([690 327 260 153 198 182 553 201 195 240], shape=(10,), dtype=int32)\n",
      "tf.Tensor([0 1 2 3 4 5 6 7 8 9], shape=(10,), dtype=int32) tf.Tensor([173  83  65  35  49  44 138  50  49  60], shape=(10,), dtype=int32)\n",
      "tf.Tensor([109], shape=(1,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "print(y_tl, count_tl)\n",
    "print(y_testl, count_testl)\n",
    "print(unique_with_counts(count_unused)[2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Modelling part\n",
    "####  creating the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg19\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 250, 250, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 250, 250, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 250, 250, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 125, 125, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 125, 125, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 125, 125, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 62, 62, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 62, 62, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 62, 62, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 62, 62, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv4 (Conv2D)       (None, 62, 62, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 31, 31, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 31, 31, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 31, 31, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 31, 31, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv4 (Conv2D)       (None, 31, 31, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 15, 15, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 15, 15, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 15, 15, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 15, 15, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv4 (Conv2D)       (None, 15, 15, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20,024,384\n",
      "Trainable params: 0\n",
      "Non-trainable params: 20,024,384\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "base_model = VGG19(include_top = False,\n",
    "                   classes = 10, \n",
    "                   input_shape = (PIXEL_SIZE, PIXEL_SIZE, 3))\n",
    "\n",
    "base_model.trainable = False\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer for the model. \n",
    "## data augmentation counteracting the small number of paintings\n",
    "data_augmentation = Sequential([\n",
    "    RandomFlip('horizontal'),\n",
    "    RandomFlip('vertical'),\n",
    "    RandomRotation(0.2),\n",
    "    RandomZoom(0.1),\n",
    "    RandomTranslation(0.1, 0.1),\n",
    "])\n",
    "prediction = Sequential([\n",
    "    Flatten(),\n",
    "    Dense(512),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.1),\n",
    "    Dense(512),\n",
    "    Dense(10, activation = 'softmax'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the model from basemodel and the other layers.\n",
    "inputs = Input(shape=(PIXEL_SIZE, PIXEL_SIZE, 3))\n",
    "x = data_augmentation(inputs)\n",
    "x = preprocess_input(x)\n",
    "x = base_model(x)\n",
    "outputs = prediction(x)\n",
    "model = Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 250, 250, 3)]     0         \n",
      "                                                                 \n",
      " sequential (Sequential)     (None, 250, 250, 3)       0         \n",
      "                                                                 \n",
      " tf.__operators__.getitem (S  (None, 250, 250, 3)      0         \n",
      " licingOpLambda)                                                 \n",
      "                                                                 \n",
      " tf.nn.bias_add (TFOpLambda)  (None, 250, 250, 3)      0         \n",
      "                                                                 \n",
      " vgg19 (Functional)          (None, 7, 7, 512)         20024384  \n",
      "                                                                 \n",
      " sequential_1 (Sequential)   (None, 10)                13115402  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 33,139,786\n",
      "Trainable params: 13,114,378\n",
      "Non-trainable params: 20,025,408\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# compile the model\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "188/188 [==============================] - 488s 3s/step - loss: 30.5106 - accuracy: 0.2147 - val_loss: 68.1188 - val_accuracy: 0.0670\n",
      "Epoch 2/100\n",
      "188/188 [==============================] - 469s 2s/step - loss: 23.0747 - accuracy: 0.2544 - val_loss: 77.3822 - val_accuracy: 0.0657\n",
      "Epoch 3/100\n",
      "188/188 [==============================] - 470s 3s/step - loss: 21.0841 - accuracy: 0.2741 - val_loss: 14.7576 - val_accuracy: 0.0737\n",
      "Epoch 4/100\n",
      "188/188 [==============================] - 464s 2s/step - loss: 19.6606 - accuracy: 0.3014 - val_loss: 32.7968 - val_accuracy: 0.0912\n",
      "Epoch 5/100\n",
      "188/188 [==============================] - 469s 2s/step - loss: 19.0671 - accuracy: 0.3061 - val_loss: 40.7589 - val_accuracy: 0.0657\n",
      "Epoch 6/100\n",
      "188/188 [==============================] - 471s 3s/step - loss: 18.7233 - accuracy: 0.3124 - val_loss: 7.2874 - val_accuracy: 0.1032\n",
      "Epoch 7/100\n",
      "188/188 [==============================] - 466s 2s/step - loss: 18.5880 - accuracy: 0.3104 - val_loss: 4.0481 - val_accuracy: 0.2842\n",
      "Epoch 8/100\n",
      "188/188 [==============================] - 472s 3s/step - loss: 18.3582 - accuracy: 0.3021 - val_loss: 37.2932 - val_accuracy: 0.0697\n",
      "Epoch 9/100\n",
      "173/188 [==========================>...] - ETA: 32s - loss: 18.2175 - accuracy: 0.3107"
     ]
    }
   ],
   "source": [
    "## training the model\n",
    "epochs = 100\n",
    "batch_size = 16\n",
    "\n",
    "early_stopping = EarlyStopping(patience = 20, \n",
    "                               verbose = 2, \n",
    "                               restore_best_weights = True)\n",
    "    \n",
    "history = model.fit(training_images,\n",
    "                    training_labels,\n",
    "                    validation_data = (testing_images, testing_labels),\n",
    "                    class_weight = class_weights,\n",
    "                    epochs = epochs,\n",
    "                    batch_size = batch_size,\n",
    "                    callbacks = early_stopping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(os.path.join(PATH_TRAINING,\"image_classifier_BvD_withVGG19basemodel.model\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.load_model(os.path.join(PATH_TRAINING,\"image_classifier_BvD_withVGG19basemodel.model\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check one prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_painting = painting(\"local DB\",random.choice(unused_paintings))\n",
    "print(test_painting.id)\n",
    "test_ndarray = cv.resize(test_painting.ndarray, dsize=(PIXEL_SIZE,PIXEL_SIZE)\n",
    "                               ,interpolation=cv.INTER_CUBIC)\n",
    "test_ndarray = test_ndarray/255\n",
    "temp_array = np.zeros((1,PIXEL_SIZE,PIXEL_SIZE,3))\n",
    "print(temp_array.shape)\n",
    "temp_array[0] = test_ndarray\n",
    "prediction = model.predict(temp_array)\n",
    "np.set_printoptions(suppress=True)\n",
    "print(np.round(prediction, 4))\n",
    "print(np.argmax(prediction))\n",
    "index = np.argmax(prediction)\n",
    "print(f\"Prediction is {class_names[index]}\")\n",
    "imgplot = plt.imshow(test_painting.ndarray)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e14a3a6e153099239264c79b5d200514d7b1165346dc721db63200c7fde2b3c3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
