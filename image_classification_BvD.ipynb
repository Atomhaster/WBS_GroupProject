{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from keras import datasets, layers, models\n",
    "import os, random\n",
    "\n",
    "from modules.painting import painting\n",
    "from modules.database import database as db\n",
    "# from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "from keras.applications import ResNet50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Control Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting path variables for Working Directory and folder to save and load\n",
    "#   models from.\n",
    "wd = os.getcwd()\n",
    "WD_PATH =  os.path.abspath(wd)\n",
    "PATH_TRAINING = os.path.join(WD_PATH, \"model_training\")\n",
    "\n",
    "# # initializing the database object\n",
    "gallery = db()\n",
    "\n",
    "# Size of the pictures when reduced in size\n",
    "PIXEL_SIZE = 256\n",
    "\n",
    "# the proportions to split the available paintings in training\n",
    "# and testing.\n",
    "keep_unused = 10\n",
    "prop_train = 0.8\n",
    "prop_test = 0.2\n",
    "\n",
    "# selection  of artists to use. (our ids)\n",
    "# 3,4,10 = Pierre-Auguste Renoir, Francisco Goya, Paul Gauguin\n",
    "artists = [1,2,3,4,5,6,7,8,9,10]\n",
    "count_artists = len(artists)\n",
    "\n",
    "# defining labels in a list\n",
    "class_names = []\n",
    "for k in artists:\n",
    "    class_names.append(gallery.get_artist(k)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the available paintings per artist.\n",
    "num_paintings = np.zeros(count_artists,np.int16)\n",
    "for j in range(count_artists):\n",
    "    num_paintings[j] = len(gallery.get_paintingids_from_artist(artists[j]))\n",
    "print(num_paintings)\n",
    "# Total number of paintings available\n",
    "TOTAL_PAINTINGS = sum(num_paintings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights for the model:\n",
    "class_temp = TOTAL_PAINTINGS / num_paintings\n",
    "class_weights = {}\n",
    "for i in range(count_artists):\n",
    "    class_weights[i] = class_temp[i]\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding the sizes needed as collection arrays for the input data\n",
    "SIZE_P_TRAINING = 0\n",
    "training_size_paintings = num_paintings.copy()\n",
    "SIZE_P_TESTING = 0\n",
    "testing_size_paintings = num_paintings.copy()\n",
    "for i in range(count_artists):\n",
    "    train_count = int((num_paintings[i] - keep_unused)  * prop_train)\n",
    "    test_count = int((num_paintings[i] - keep_unused)  * prop_test)\n",
    "    # print(sum((train_count,test_count,10)),training_size_paintings[i])\n",
    "    SIZE_P_TRAINING += train_count\n",
    "    SIZE_P_TESTING += test_count\n",
    "    training_size_paintings[i] = train_count\n",
    "    testing_size_paintings[i] = test_count\n",
    "print(\"total train paintings:\", SIZE_P_TRAINING)\n",
    "print(\"total test paintings:\", SIZE_P_TESTING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## creating arrays to hold the pictures taken from the DB\n",
    "training_images = np.zeros((SIZE_P_TRAINING,PIXEL_SIZE,PIXEL_SIZE,3))\n",
    "index_training = 0\n",
    "skip_count_training = 0\n",
    "testing_images = np.zeros((SIZE_P_TESTING,PIXEL_SIZE,PIXEL_SIZE,3))\n",
    "index_testing = 0\n",
    "skip_count_testing = 0\n",
    "# creating the arrays to hold labels. In this case they are the artist ids.\n",
    "training_labels = np.zeros((SIZE_P_TRAINING,count_artists),dtype=int)\n",
    "testing_labels = np.zeros((SIZE_P_TESTING,count_artists),dtype=int)\n",
    "unused_paintings = np.array(\n",
    "    [0]*(TOTAL_PAINTINGS-SIZE_P_TESTING-SIZE_P_TRAINING)\n",
    "    ,dtype=int)\n",
    "# checking if the numbers add up.\n",
    "print(training_images.shape)\n",
    "print(testing_images.shape)\n",
    "print(training_labels.shape)\n",
    "print(testing_labels.shape)\n",
    "print(unused_paintings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filling the arrays with picture arrays. They will be resized according\n",
    "# to the pixel_size value\n",
    "unused_index = 0\n",
    "for i in range(count_artists):\n",
    "    # loading all ids from the artist\n",
    "    ids = gallery.get_paintingids_from_artist(artists[i])\n",
    "    # shuffle the ids to get random order for selection\n",
    "    random.seed(1983)\n",
    "    random.shuffle(ids)\n",
    "    print(\"New Artist\")\n",
    "    \n",
    "    # getting the numbers for the current artist:\n",
    "    _str = training_size_paintings[i]\n",
    "    _ste = testing_size_paintings[i]\n",
    "    # slicing the ids for training and testing of artist with id i+1\n",
    "    ids_training = ids[ : _str]\n",
    "    ids_testing = ids[_str : _str + _ste]\n",
    "    ids_unused = ids[ _str + _ste : ]\n",
    "    \n",
    "    # collecting the ids of the unused paintings\n",
    "    for l, f in zip(range(unused_index,unused_index+len(ids_unused))\n",
    "                    , ids_unused):\n",
    "        unused_paintings[l] = f[0]\n",
    "    unused_index += len(ids_unused)\n",
    "    \n",
    "    # retrieving the paintings from the db, resizing them and collecting\n",
    "    # them in the training_images array while also filling the labels\n",
    "    for k in ids_training:\n",
    "        temp_p = painting(\"local DB\", id=k[0])\n",
    "        temp_p_res = cv.resize(temp_p.ndarray, dsize=(PIXEL_SIZE,PIXEL_SIZE)\n",
    "                               ,interpolation=cv.INTER_CUBIC)\n",
    "        if temp_p_res.shape == (PIXEL_SIZE,PIXEL_SIZE,3):\n",
    "            training_images[index_training] = temp_p_res\n",
    "            training_labels[index_training,i] = 1\n",
    "            index_training += 1\n",
    "        else:\n",
    "            skip_count_training += 1\n",
    "            \n",
    "    # retrieving the paintings from the db, resizing them and collecting\n",
    "    # them in the testing_images array while also filling the labels\n",
    "    for j in ids_testing:\n",
    "        temp_p = painting(\"local DB\", id=j[0])\n",
    "        temp_p_res = cv.resize(temp_p.ndarray, dsize=(PIXEL_SIZE,PIXEL_SIZE)\n",
    "                               ,interpolation=cv.INTER_CUBIC)\n",
    "        if temp_p_res.shape == (PIXEL_SIZE,PIXEL_SIZE,3):\n",
    "            testing_images[index_testing] = temp_p_res\n",
    "            testing_labels[index_testing,i] = 1\n",
    "            index_testing += 1\n",
    "        else:\n",
    "            skip_count_testing += 1\n",
    "\n",
    "# ## dropping the last few array positions of testing images, which where \n",
    "# ## not filled.\n",
    "testing_images = testing_images[:index_testing,:,:,:]\n",
    "testing_labels = testing_labels[:index_testing,:]\n",
    "# ## dropping the last few array positions of training images, which where \n",
    "# ## not filled.\n",
    "training_images = training_images[:index_training,:,:,:]\n",
    "training_labels = training_labels[:index_training,:]\n",
    "\n",
    "# # the pixels on an image are rescaled from 0-255 to 0-1 \n",
    "training_images, testing_images = training_images/255, testing_images/255 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffeling all input data.\n",
    "index_testing = list(range(testing_images.shape[0]))\n",
    "index_training = list(range(training_images.shape[0]))\n",
    "random.seed(1983)\n",
    "random.shuffle(index_testing)\n",
    "testing_images_shuffled = testing_images.copy()\n",
    "testing_labels_shuffled = testing_labels.copy()\n",
    "for i,j in zip(index_testing, range(testing_images.shape[0])):\n",
    "    testing_images_shuffled[i] = testing_images[j]\n",
    "    testing_labels_shuffled[i] = testing_labels[j]\n",
    "\n",
    "random.shuffle(index_training)\n",
    "training_images_shuffled = training_images.copy()\n",
    "training_labels_shuffled = training_labels.copy()\n",
    "for i,j in zip(index_training, range(training_images.shape[0])):\n",
    "    training_images_shuffled[i] = training_images[j]\n",
    "    training_labels_shuffled[i] = training_labels[j]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking data shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(training_images.shape)\n",
    "print(testing_images.shape)\n",
    "print(training_labels.shape)\n",
    "print(testing_labels.shape)\n",
    "print(type(training_labels[185:200]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(training_labels[500:510])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(np.unique(training_labels))\n",
    "# print(type(training_labels[21]))\n",
    "# print(training_labels[21].shape)\n",
    "# print(training_labels[500])\n",
    "# print(training_labels[:,0].shape)\n",
    "# print(type(training_labels[:,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, (2, 2), input_shape=(PIXEL_SIZE,PIXEL_SIZE,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    " \n",
    "model.add(Conv2D(32, (2, 2)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    " \n",
    "model.add(Conv2D(64, (2, 2)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    " \n",
    "model.add(Flatten())\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## different version\n",
    "model = Sequential()\n",
    "model.add(Conv2D(64,(3,3),padding='same',input_shape=(PIXEL_SIZE, PIXEL_SIZE , 3)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(128,(3 , 3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Flatten ())\n",
    "  \n",
    "## fully connected layer\n",
    "for i in range(0):\n",
    "    units=256//(i+1)\n",
    "    if units<32:\n",
    "        break\n",
    "    \n",
    "    if i ==0:\n",
    "        input_dim=model.output_shape[1]\n",
    "        \n",
    "    else:\n",
    "        input_dim=units\n",
    "        \n",
    "    print(input_dim)\n",
    "    \n",
    "    # Add the Dense Layer along with activation and batch normalization\n",
    "    \n",
    "    dense_layer=Dense(units=units,input_dim=input_dim,)\n",
    "    \n",
    "    # Add the Leaky ReLU activation layer\n",
    "    \n",
    "    dense_layer=Dense(units=64,input_dim=model.output_shape[1])\n",
    "    activation_layer=Activation('relu')\n",
    "    batch_norm_layer = BatchNormalization()\n",
    "    model.add(dense_layer)\n",
    "    model.add(activation_layer)\n",
    "    model.add(batch_norm_layer)\n",
    "   \n",
    "## Add Dropout Layer\n",
    "dropout_fraction=0.5\n",
    "dropout_layer=Dropout(dropout_fraction)\n",
    "\n",
    "## Add the dense layer for final output\n",
    "dense_output=Dense(units=count_artists, input_dim=model.output_shape[1], activation='softmax')\n",
    "# activation_output = Activation('sigmoid')\n",
    "\n",
    "model.add(dropout_layer)\n",
    "model.add(dense_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Adam(learning_rate=0.001) # learning rate\n",
    "\n",
    "# Compile the Model\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(training_images_shuffled,training_labels_shuffled\n",
    "                    ,batch_size=64\n",
    "                    ,epochs=3\n",
    "                    ,validation_data=(testing_images,testing_labels)\n",
    "                    , class_weight= class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss,accuracy = model.evaluate(testing_images, testing_labels)\n",
    "print(f\"Loss: {loss}\")\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(os.path.join(PATH_TRAINING,\"image_classifier_BvDtest.model\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = models.load_model(os.path.join(PATH_TRAINING,\"image_classifier_BvDtest.model\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check one prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_painting = painting(\"local DB\",random.choice(unused_paintings))\n",
    "# print(test_painting.id)\n",
    "# test_ndarray = cv.resize(test_painting.ndarray, dsize=(PIXEL_SIZE,PIXEL_SIZE)\n",
    "#                                ,interpolation=cv.INTER_CUBIC)\n",
    "# test_ndarray = test_ndarray/255\n",
    "# temp_array = np.zeros((1,PIXEL_SIZE,PIXEL_SIZE,3))\n",
    "# print(temp_array.shape)\n",
    "# temp_array[0] = test_ndarray\n",
    "# prediction = model.predict(temp_array)\n",
    "# np.set_printoptions(suppress=True)\n",
    "# print(np.round(prediction, 4))\n",
    "# print(np.argmax(prediction))\n",
    "# index = np.argmax(prediction)\n",
    "# print(f\"Prediction is {class_names[index]}\")\n",
    "\n",
    "# art_id = gallery.get_painting(test_painting.id)[1]\n",
    "# print(\"Correct is \", gallery.get_artist(art_id)[0][1])\n",
    "\n",
    "# imgplot = plt.imshow(test_painting.ndarray)\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e14a3a6e153099239264c79b5d200514d7b1165346dc721db63200c7fde2b3c3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
