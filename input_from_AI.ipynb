{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[14:52] Mohamad Al Turk\n",
    "\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(64,(3,3),padding='same',input_shape=(PIXEL_SIZE, PIXEL_SIZE , 3)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(128,(3 , 3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "# model. add ( Convolutional -Schichten (64 ,(5 ,5 ), padding = 'same' ))\n",
    "# model. add ( BatchNormalization ())\n",
    "# model. add ( Activation ('ReLU' ))\n",
    "# model. add ( MaxPooling2D (( 4 ,4 )))\n",
    "\n",
    "# model .add(Convolutional-Schichten(128 ,(7 ,7 ), padding = ' same '))\n",
    "# model .add(BatchNormalization ())\n",
    "# model .add ( Activation ('ReLU' ))\n",
    "\n",
    "model .add(Flatten ())\n",
    "  \n",
    "## fully connected layer\n",
    "for i in range(0):\n",
    "    units=256//(i+1)\n",
    "    if units<32:\n",
    "        break\n",
    "    \n",
    "    if i ==0:\n",
    "        input_dim=model.output_shape[1]\n",
    "        \n",
    "    else:\n",
    "        input_dim=units\n",
    "        \n",
    "    print(input_dim)\n",
    "    \n",
    "    # Add the Dense Layer along with activation and batch normalization\n",
    "    \n",
    "    dense_layer=Dense(units=units,input_dim=input_dim,)\n",
    "    \n",
    "    # Add the Leaky ReLU activation layer\n",
    "    \n",
    "   \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "dense_layer=Dense(units=64,input_dim=model.output_shape[1])\n",
    "activation_layer=Activation('relu')\n",
    "batch_norm_layer = BatchNormalization()\n",
    "\n",
    "model.add(dense_layer)\n",
    "model.add(activation_layer)\n",
    "model.add(batch_norm_layer)\n",
    "\n",
    "## Add Dropout Layer\n",
    "dropout_fraction=0.5\n",
    "dropout_layer=Dropout(dropout_fraction)\n",
    "\n",
    "## Add the dense layer for final output\n",
    "dense_output=Dense(units=1, input_dim=model.output_shape[1], activation='sigmoid')\n",
    "activation_output = Activation('sigmoid')\n",
    "\n",
    "model.add(dropout_layer)\n",
    "model.add(dense_output)\n",
    "# model. add ( Activation ('Sigmoid' ))\n",
    "\n",
    "opt = Adam(lr=0.0001) # learning rate\n",
    "\n",
    "# Compile the Model\n",
    "model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "```\n",
    "\n",
    "In diesem Beispiel wurden folgende Optimierungen vorgenommen:\n",
    "\n",
    "- Die Anzahl der Filter in den Convolutional-Schichten wurde erhöht: Es wurden 64 und 128 Filter verwendet.\n",
    "- Batch-Normalisierung wurde hinzugefügt: Dies trägt dazu bei, dass das Modell schneller konvergiert.\n",
    "- Eine weitere Convolutional-Schicht wurde auskommentiert um das Modell einfacher zu halten.\n",
    "- Ein Schleifenblock zur Erstellung von Dense-Layern mit unterschiedlichen Größen und Aktivierungsfunktionen wie LeakyReLU oder ELU anstelle von ReLU wurde hinzugefügt (auskommentiert). Dies kann dazu beitragen, dass das Modell komplexere Beziehungen zwischen den Merkmalen besser erfasst.\n",
    " \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e14a3a6e153099239264c79b5d200514d7b1165346dc721db63200c7fde2b3c3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
